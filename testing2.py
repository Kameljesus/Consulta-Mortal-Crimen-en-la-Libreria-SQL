import requests
from bs4 import BeautifulSoup
import time

base_url = "https://books.toscrape.com/"
fallidos = []  # Aqu√≠ guardamos los links que fallaron

def obtener_html_con_reintentos(url, max_reintentos=5, espera=3):
    intentos = 0
    while intentos < max_reintentos:
        try:
            r = requests.get(url, timeout=10)
            r.raise_for_status()
            return r.text  # Devuelve el HTML si todo sale bien
        except requests.exceptions.RequestException as e:
            intentos += 1
            print(f"‚ùå Error en {url} ‚Üí {e}")
            print(f"Reintentando ({intentos}/{max_reintentos})...")
            time.sleep(espera)
    print(f"üíÄ Fallo final: {url}")
    return None  # Si falla todas las veces

# Iteramos por las 50 p√°ginas del cat√°logo
for i in range(1, 51):  # p√°ginas 1 a 50
    print()
    print(f"Procesando p√°gina {i}...")
    print()
    url = f"{base_url}catalogue/page-{i}.html"

    html = obtener_html_con_reintentos(url)
    if html is None:
        print(f"‚ùå Fall√≥ la p√°gina {i}, la saltamos.")
        fallidos.append(url_libro)
        continue

    soup = BeautifulSoup(html, 'html.parser')
    libros = soup.find_all('article', class_='product_pod')

    for libro in libros:
        # T√≠tulo
        titulo = libro.h3.a['title']
        
        # Enlace a la p√°gina del libro
        link = libro.h3.a['href']
        # A veces los links son relativos (ej: '../../../...'), arreglamos eso:
        link = link.replace('../../../', '')
        url_libro = base_url + "catalogue/" + link

        html_libro = obtener_html_con_reintentos(url_libro)
        if html_libro is None:
            print(f"‚ùå Fall√≥: {titulo} (no se pudo recuperar HTML)")
            fallidos.append(url_libro)
            continue

        soup_libro = BeautifulSoup(html_libro, 'html.parser')

        try:
            # Precio
            precio = soup_libro.find('p', class_='price_color').text

            # Categor√≠a
            categoria = soup_libro.find('ul', class_='breadcrumb').find_all('a')[2].text.strip()

            # Calificaci√≥n
            calificacion_tag = soup_libro.find('p', class_='star-rating')
            clases = calificacion_tag['class']
            calificacion = [c for c in clases if c != 'star-rating'][0]

            # Mostramos por consola
            print(f"{titulo} | {precio} | {categoria} | {calificacion} Stars")

        except Exception as e:
            print(f"‚ö†Ô∏è Error de parsing interno en {url_libro} ‚Üí {e}")
            fallidos.append(url_libro)

        time.sleep(10)  # pausa entre libros

# Al final mostramos los fallidos
print("\nüîÅ Libros que fallaron:")
for f in fallidos:
    print(f)
